
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Temporal-Difference (TD) Learning &#8212; Atelier : Apprentissage par Renforcement (RL)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'theorie/04_td_learning';</script>
    <link rel="canonical" href="/RL-workshop/theorie/04_td_learning.html" />
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="06 — Approximation de Fonctions &amp; Généralisation" href="05_function_approximation.html" />
    <link rel="prev" title="Monte Carlo Methods" href="03_monte_carlo.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Atelier : Apprentissage par Renforcement (RL) - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Atelier : Apprentissage par Renforcement (RL) - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Introduction à l’Apprentissage par Renforcement
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_concepts_de_base.html">1. Concepts fondamentaux du RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_dynamic_programming.html">2. Dynamic Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_monte_carlo.html">3. Monte Carlo methods</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">4. TD, SARSA, Q-Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_function_approximation.html">5. Approximation &amp; généralisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_policy_gradients.html">6. Policy Gradient methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_deep_rl.html">7. Deep Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pratiques/coding_session.html">8. Etude de cas: CartPole</a></li>
<li class="toctree-l1"><a class="reference internal" href="../applications.html">9. Applications avancées du RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ressources.html">Références et ressources</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/theorie/04_td_learning.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Temporal-Difference (TD) Learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#principe-fondamental-du-td-learning">1. Principe fondamental du TD Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#td-error">1.1 TD Error</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#td-0-prediction">2. TD(0) — Prediction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithme">2.1 Algorithme</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#proprietes-de-convergence">2.2 Propriétés de convergence</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#td-control-sarsa-on-policy">3. TD Control : SARSA (On-policy)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#principe">3.1 Principe</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">3.2 Algorithme</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#caracteristiques">3.3 Caractéristiques</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#q-learning-off-policy">4. Q-Learning (Off-policy)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">4.1 Principe</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">4.2 Algorithme</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convergence">4.3 Convergence</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sarsa-vs-q-learning">5. SARSA vs Q-Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparaison-theorique">5.1 Comparaison théorique</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exemple-illustratif-cliff-walking">5.2 Exemple illustratif : Cliff Walking</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#td-et-eligibility-traces">6. TD(λ) et Eligibility Traces</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#n-step-td">6.1 N-step TD</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#td-avec-eligibility-traces">6.2 TD(λ) avec eligibility traces</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercices-theoriques">7. Exercices théoriques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercice-1-calcul-de-td-error">Exercice 1 — Calcul de TD error</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercice-2-sarsa-step-by-step">Exercice 2 — SARSA step by step</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercice-3-q-learning-vs-sarsa">Exercice 3 — Q-Learning vs SARSA</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercice-4-sequence-de-mises-a-jour-td">Exercice 4 — Séquence de mises à jour TD</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercice-5-convergence-avec-learning-rate">Exercice 5 — Convergence avec learning rate</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#avantages-et-limitations">8. Avantages et limitations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#avantages-des-methodes-td">8.1 Avantages des méthodes TD</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations">8.2 Limitations</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="temporal-difference-td-learning">
<h1>Temporal-Difference (TD) Learning<a class="headerlink" href="#temporal-difference-td-learning" title="Link to this heading">#</a></h1>
<p>Les méthodes <strong>Temporal-Difference (TD)</strong> combinent les avantages de <strong>Monte Carlo</strong> (model-free, apprentissage par expérience) et de <strong>Dynamic Programming</strong> (bootstrapping, pas besoin d’épisodes complets).</p>
<hr class="docutils" />
<section id="principe-fondamental-du-td-learning">
<h2>1. Principe fondamental du TD Learning<a class="headerlink" href="#principe-fondamental-du-td-learning" title="Link to this heading">#</a></h2>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>L’idée centrale du TD</strong></p>
<p>Au lieu d’attendre le return complet <span class="math notranslate nohighlight">\(G_t\)</span> (comme MC), TD utilise une <strong>estimation bootstrap</strong> :</p>
<div class="math notranslate nohighlight">
\[\text{TD target} = R_{t+1} + \gamma V(S_{t+1})\]</div>
<p><strong>Mise à jour TD :</strong></p>
<div class="math notranslate nohighlight">
\[V(S_t) \leftarrow V(S_t) + \alpha \underbrace{\left[R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\right]}_{\text{TD error } \delta_t}\]</div>
<p><strong>Comparaison :</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Méthode</p></th>
<th class="head"><p>Cible (target)</p></th>
<th class="head"><p>Bootstrap</p></th>
<th class="head"><p>Épisode complet</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>MC</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(G_t\)</span> (return réel)</p></td>
<td><p>❌ Non</p></td>
<td><p>✅ Oui</p></td>
</tr>
<tr class="row-odd"><td><p><strong>TD</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(R_{t+1} + \gamma V(S_{t+1})\)</span></p></td>
<td><p>✅ Oui</p></td>
<td><p>❌ Non</p></td>
</tr>
<tr class="row-even"><td><p><strong>DP</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(\sum_{s',r} P(s',r|s,a)[r + \gamma V(s')]\)</span></p></td>
<td><p>✅ Oui</p></td>
<td><p>❌ Non</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<section id="td-error">
<h3>1.1 TD Error<a class="headerlink" href="#td-error" title="Link to this heading">#</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>TD Error (erreur temporelle)</strong></p>
<div class="math notranslate nohighlight">
\[\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\]</div>
<p><strong>Interprétation :</strong></p>
<ul class="simple">
<li><p>Si <span class="math notranslate nohighlight">\(\delta_t &gt; 0\)</span> : la reward + valeur future &gt; estimation actuelle → <strong>sous-estimation</strong></p></li>
<li><p>Si <span class="math notranslate nohighlight">\(\delta_t &lt; 0\)</span> : la reward + valeur future &lt; estimation actuelle → <strong>surestimation</strong></p></li>
<li><p>Le TD error est le <strong>signal d’apprentissage</strong> qui guide la mise à jour</p></li>
</ul>
<p><strong>Propriété importante :</strong> <span class="math notranslate nohighlight">\(\mathbb{E}[\delta_t] = 0\)</span> à la convergence (quand <span class="math notranslate nohighlight">\(V = V^{\pi}\)</span>)</p>
</div>
</section>
</section>
<hr class="docutils" />
<section id="td-0-prediction">
<h2>2. TD(0) — Prediction<a class="headerlink" href="#td-0-prediction" title="Link to this heading">#</a></h2>
<section id="algorithme">
<h3>2.1 Algorithme<a class="headerlink" href="#algorithme" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Algorithme : TD(0) Prediction
────────────────────────────────────────────────
Entrée : policy π à évaluer, learning rate α
Sortie : V ≈ V^π

// Initialisation
pour chaque s ∈ S faire :
    V(s) ← 0  (ou valeurs arbitraires)
fin pour

// Apprentissage
répéter pour chaque épisode :
    Initialiser S (état de départ)
    
    répéter pour chaque step de l&#39;épisode :
        A ← action donnée par π(·|S)
        Exécuter A, observer R, S&#39;
        
        // Mise à jour TD(0)
        V(S) ← V(S) + α[R + γ·V(S&#39;) - V(S)]
        
        S ← S&#39;
    jusqu&#39;à S terminal
    
jusqu&#39;à convergence

retourner V
</pre></div>
</div>
</section>
<section id="proprietes-de-convergence">
<h3>2.2 Propriétés de convergence<a class="headerlink" href="#proprietes-de-convergence" title="Link to this heading">#</a></h3>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Théorème de convergence TD(0)</strong></p>
<p>Sous les conditions :</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\sum_{t=1}^{\infty} \alpha_t = \infty\)</span> (somme infinie)</p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_{t=1}^{\infty} \alpha_t^2 &lt; \infty\)</span> (somme des carrés finie)</p></li>
<li><p>Tous les états visités infiniment souvent</p></li>
</ol>
<p>Alors TD(0) converge vers <span class="math notranslate nohighlight">\(V^{\pi}\)</span> avec probabilité 1.</p>
<p><strong>Exemple de learning rate valide :</strong> <span class="math notranslate nohighlight">\(\alpha_t = \frac{1}{t}\)</span> ou <span class="math notranslate nohighlight">\(\alpha_t = \frac{1}{\sqrt{t}}\)</span></p>
<p><strong>Pratique courante :</strong> <span class="math notranslate nohighlight">\(\alpha\)</span> constant (e.g., 0.1) pour adaptation continue</p>
</div>
</section>
</section>
<hr class="docutils" />
<section id="td-control-sarsa-on-policy">
<h2>3. TD Control : SARSA (On-policy)<a class="headerlink" href="#td-control-sarsa-on-policy" title="Link to this heading">#</a></h2>
<section id="principe">
<h3>3.1 Principe<a class="headerlink" href="#principe" title="Link to this heading">#</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>SARSA = State-Action-Reward-State-Action</strong></p>
<p>SARSA apprend <span class="math notranslate nohighlight">\(Q^{\pi}(s,a)\)</span> en suivant <strong>la même policy</strong> <span class="math notranslate nohighlight">\(\pi\)</span> pour :</p>
<ul class="simple">
<li><p>Choisir l’action <span class="math notranslate nohighlight">\(A_t\)</span> en <span class="math notranslate nohighlight">\(S_t\)</span></p></li>
<li><p>Choisir l’action suivante <span class="math notranslate nohighlight">\(A_{t+1}\)</span> en <span class="math notranslate nohighlight">\(S_{t+1}\)</span></p></li>
</ul>
<p><strong>Mise à jour :</strong></p>
<div class="math notranslate nohighlight">
\[Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]\]</div>
<p><strong>On-policy :</strong> La policy d’exploration et d’évaluation est la <strong>même</strong> (typiquement ε-greedy).</p>
</div>
</section>
<section id="id1">
<h3>3.2 Algorithme<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Algorithme : SARSA (on-policy TD Control)
────────────────────────────────────────────────
Entrée : learning rate α, exploration ε
Sortie : Q ≈ Q*, π ≈ π*

// Initialisation
pour chaque s ∈ S, a ∈ A faire :
    Q(s,a) ← 0  (ou arbitraire)
fin pour

// Apprentissage
répéter pour chaque épisode :
    Initialiser S
    Choisir A depuis S avec policy ε-greedy basée sur Q
    
    répéter pour chaque step :
        Exécuter A, observer R, S&#39;
        
        Choisir A&#39; depuis S&#39; avec policy ε-greedy basée sur Q
        
        // Mise à jour SARSA
        Q(S,A) ← Q(S,A) + α[R + γ·Q(S&#39;,A&#39;) - Q(S,A)]
        
        S ← S&#39;
        A ← A&#39;
        
    jusqu&#39;à S terminal
    
jusqu&#39;à convergence

retourner Q
</pre></div>
</div>
</section>
<section id="caracteristiques">
<h3>3.3 Caractéristiques<a class="headerlink" href="#caracteristiques" title="Link to this heading">#</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Propriétés de SARSA</strong></p>
<p>✅ <strong>On-policy</strong> : Apprend la valeur de la policy qu’il suit (ε-greedy)</p>
<p>✅ <strong>Convergence garantie</strong> : Vers <span class="math notranslate nohighlight">\(Q^{\pi_{\varepsilon\text{-greedy}}}\)</span> sous conditions standards</p>
<p>✅ <strong>Prudent</strong> : Tient compte de l’exploration dans l’apprentissage</p>
<p>❌ <strong>Pas optimal</strong> : Converge vers la policy ε-greedy, pas la policy optimale pure</p>
<p><strong>Astuce pratique :</strong> Réduire <span class="math notranslate nohighlight">\(\varepsilon\)</span> progressivement pour s’approcher de l’optimal</p>
</div>
</section>
</section>
<hr class="docutils" />
<section id="q-learning-off-policy">
<h2>4. Q-Learning (Off-policy)<a class="headerlink" href="#q-learning-off-policy" title="Link to this heading">#</a></h2>
<section id="id2">
<h3>4.1 Principe<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Q-Learning : apprentissage off-policy</strong></p>
<p>Q-Learning apprend <strong>directement</strong> <span class="math notranslate nohighlight">\(Q^*\)</span> (optimal) indépendamment de la policy suivie.</p>
<p><strong>Mise à jour :</strong></p>
<div class="math notranslate nohighlight">
\[Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[R_{t+1} + \gamma \max_{a} Q(S_{t+1}, a) - Q(S_t, A_t)\right]\]</div>
<p><strong>Différence clé avec SARSA :</strong></p>
<ul class="simple">
<li><p><strong>SARSA</strong> : utilise <span class="math notranslate nohighlight">\(Q(S', A')\)</span> (action <strong>réellement choisie</strong>)</p></li>
<li><p><strong>Q-Learning</strong> : utilise <span class="math notranslate nohighlight">\(\max_a Q(S', a)\)</span> (action <strong>optimale</strong>)</p></li>
</ul>
<p><strong>Off-policy :</strong> La policy d’exploration (ε-greedy) et la policy cible (greedy) sont <strong>différentes</strong>.</p>
</div>
</section>
<section id="id3">
<h3>4.2 Algorithme<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Algorithme : Q-Learning (off-policy TD Control)
────────────────────────────────────────────────
Entrée : learning rate α, exploration ε
Sortie : Q ≈ Q*, π* greedy basée sur Q

// Initialisation
pour chaque s ∈ S, a ∈ A faire :
    Q(s,a) ← 0  (ou arbitraire)
fin pour

// Apprentissage
répéter pour chaque épisode :
    Initialiser S
    
    répéter pour chaque step :
        Choisir A depuis S avec policy ε-greedy basée sur Q
        Exécuter A, observer R, S&#39;
        
        // Mise à jour Q-Learning (MAX bootstrap)
        Q(S,A) ← Q(S,A) + α[R + γ·max_a Q(S&#39;,a) - Q(S,A)]
        
        S ← S&#39;
        
    jusqu&#39;à S terminal
    
jusqu&#39;à convergence

// Extraire policy optimale
pour chaque s ∈ S faire :
    π(s) ← argmax_a Q(s,a)
fin pour

retourner Q, π
</pre></div>
</div>
</section>
<section id="convergence">
<h3>4.3 Convergence<a class="headerlink" href="#convergence" title="Link to this heading">#</a></h3>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Théorème de convergence Q-Learning</strong></p>
<p>Sous les conditions :</p>
<ol class="arabic simple">
<li><p>Tous les états et actions visités infiniment souvent</p></li>
<li><p>Learning rate satisfait les conditions de Robbins-Monro</p></li>
<li><p>Q-values bornées</p></li>
</ol>
<p>Alors Q-Learning converge vers <span class="math notranslate nohighlight">\(Q^*\)</span> avec probabilité 1.</p>
<p><strong>Résultat remarquable :</strong> Converge vers l’optimal même en explorant avec ε-greedy !</p>
</div>
</section>
</section>
<hr class="docutils" />
<section id="sarsa-vs-q-learning">
<h2>5. SARSA vs Q-Learning<a class="headerlink" href="#sarsa-vs-q-learning" title="Link to this heading">#</a></h2>
<section id="comparaison-theorique">
<h3>5.1 Comparaison théorique<a class="headerlink" href="#comparaison-theorique" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Critère</p></th>
<th class="head"><p>SARSA</p></th>
<th class="head"><p>Q-Learning</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Type</strong></p></td>
<td><p>On-policy</p></td>
<td><p>Off-policy</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Cible</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(R + \gamma Q(S', A')\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(R + \gamma \max_a Q(S', a)\)</span></p></td>
</tr>
<tr class="row-even"><td><p><strong>Converge vers</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(Q^{\pi_{\varepsilon}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(Q^*\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><strong>Comportement</strong></p></td>
<td><p>Prudent</p></td>
<td><p>Optimiste</p></td>
</tr>
<tr class="row-even"><td><p><strong>Variance</strong></p></td>
<td><p>Plus faible</p></td>
<td><p>Plus élevée</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Biais</strong></p></td>
<td><p>Léger</p></td>
<td><p>Plus important initialement</p></td>
</tr>
<tr class="row-even"><td><p><strong>Usage</strong></p></td>
<td><p>Environnements dangereux</p></td>
<td><p>Maximiser performance</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="exemple-illustratif-cliff-walking">
<h3>5.2 Exemple illustratif : Cliff Walking<a class="headerlink" href="#exemple-illustratif-cliff-walking" title="Link to this heading">#</a></h3>
<div class="note admonition">
<p class="admonition-title">Problème du Cliff Walking</p>
<p><img alt="Clif walking animation" src="https://imgs.search.brave.com/ZczVz-qSmdpDpsE3yOzBVw2PezrrltI21QefkGVoUKQ/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly9neW1u/YXNpdW0uZmFyYW1h/Lm9yZy9faW1hZ2Vz/L2NsaWZmX3dhbGtp/bmcuZ2lm.gif" />
Imaginons un gridworld avec :</p>
<ul class="simple">
<li><p>État départ : en bas à gauche</p></li>
<li><p>État goal : en bas à droite</p></li>
<li><p>Falaise (cliff) : ligne du bas entre départ et goal (reward = -100)</p></li>
<li><p>Chemin sûr : contourner par le haut (plus long)</p></li>
</ul>
<p><strong>Comportement observé :</strong></p>
<ul class="simple">
<li><p><strong>SARSA</strong> : Apprend un chemin <strong>sûr</strong> (loin de la falaise) car il explore avec ε-greedy</p></li>
<li><p><strong>Q-Learning</strong> : Apprend le chemin <strong>optimal</strong> (proche de la falaise) mais risqué pendant l’apprentissage</p></li>
</ul>
<p><strong>Raison :</strong> SARSA prend en compte les erreurs d’exploration dans son apprentissage.</p>
</div>
</section>
</section>
<hr class="docutils" />
<section id="td-et-eligibility-traces">
<h2>6. TD(λ) et Eligibility Traces<a class="headerlink" href="#td-et-eligibility-traces" title="Link to this heading">#</a></h2>
<section id="n-step-td">
<h3>6.1 N-step TD<a class="headerlink" href="#n-step-td" title="Link to this heading">#</a></h3>
<div class="note admonition">
<p class="admonition-title">Généralisation : n-step returns</p>
<p>Au lieu d’un seul step (TD(0)), on peut utiliser <span class="math notranslate nohighlight">\(n\)</span> steps :</p>
<p><strong>1-step (TD(0))</strong> : <span class="math notranslate nohighlight">\(G_t^{(1)} = R_{t+1} + \gamma V(S_{t+1})\)</span></p>
<p><strong>2-step</strong> : <span class="math notranslate nohighlight">\(G_t^{(2)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 V(S_{t+2})\)</span></p>
<p><strong>n-step</strong> : <span class="math notranslate nohighlight">\(G_t^{(n)} = \sum_{k=0}^{n-1} \gamma^k R_{t+k+1} + \gamma^n V(S_{t+n})\)</span></p>
<p><strong>MC (∞-step)</strong> : <span class="math notranslate nohighlight">\(G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}\)</span></p>
<p><strong>Trade-off :</strong> <span class="math notranslate nohighlight">\(n\)</span> petit → moins de variance mais plus de biais ; <span class="math notranslate nohighlight">\(n\)</span> grand → inverse</p>
</div>
</section>
<section id="td-avec-eligibility-traces">
<h3>6.2 TD(λ) avec eligibility traces<a class="headerlink" href="#td-avec-eligibility-traces" title="Link to this heading">#</a></h3>
<div class="note admonition">
<p class="admonition-title">TD(λ) : combinaison de tous les n-step returns</p>
<p>Le paramètre <span class="math notranslate nohighlight">\(\lambda \in [0,1]\)</span> contrôle le mélange :</p>
<p><strong>λ-return :</strong></p>
<div class="math notranslate nohighlight">
\[G_t^{\lambda} = (1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_t^{(n)}\]</div>
<p><strong>Cas particuliers :</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lambda = 0\)</span> → TD(0) (bootstrap complet)</p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda = 1\)</span> → Monte Carlo (pas de bootstrap)</p></li>
</ul>
<p><strong>Eligibility trace</strong> <span class="math notranslate nohighlight">\(e_t(s)\)</span> : mémoire de quels états doivent être crédités</p>
<div class="math notranslate nohighlight">
\[\begin{split}e_t(s) = \begin{cases}
\gamma \lambda e_{t-1}(s) + 1 &amp; \text{si } s = S_t \\
\gamma \lambda e_{t-1}(s) &amp; \text{sinon}
\end{cases}\end{split}\]</div>
<p><strong>Mise à jour TD(λ) :</strong></p>
<p>Pour tous les états <span class="math notranslate nohighlight">\(s\)</span> à chaque step :</p>
<div class="math notranslate nohighlight">
\[V(s) \leftarrow V(s) + \alpha \delta_t e_t(s)\]</div>
<p>où <span class="math notranslate nohighlight">\(\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\)</span></p>
</div>
</section>
</section>
<hr class="docutils" />
<section id="exercices-theoriques">
<h2>7. Exercices théoriques<a class="headerlink" href="#exercices-theoriques" title="Link to this heading">#</a></h2>
<section id="exercice-1-calcul-de-td-error">
<h3>Exercice 1 — Calcul de TD error<a class="headerlink" href="#exercice-1-calcul-de-td-error" title="Link to this heading">#</a></h3>
<div class="tip admonition">
<p class="admonition-title">Exercice</p>
<p>Un agent observe la transition suivante :</p>
<div class="math notranslate nohighlight">
\[S_t = s_1 \xrightarrow{A_t = a_1, R_{t+1} = 3} S_{t+1} = s_2\]</div>
<p><strong>Value functions actuelles :</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(V(s_1) = 10.0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(V(s_2) = 15.0\)</span></p></li>
</ul>
<p><strong>Paramètres :</strong> <span class="math notranslate nohighlight">\(\alpha = 0.1\)</span>, <span class="math notranslate nohighlight">\(\gamma = 0.9\)</span></p>
<p><strong>Questions :</strong></p>
<p>a) Calculez le TD error <span class="math notranslate nohighlight">\(\delta_t\)</span></p>
<p>b) Quelle est la nouvelle valeur <span class="math notranslate nohighlight">\(V(s_1)\)</span> après mise à jour TD(0) ?</p>
<p>c) Si <span class="math notranslate nohighlight">\(V(s_2)\)</span> était en réalité 5.0 (au lieu de 15.0), quel serait <span class="math notranslate nohighlight">\(\delta_t\)</span> ?</p>
<p>d) Interprétez le signe des TD errors en (a) et (c)</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution — Exercice 1</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>a) TD error :</strong></p>
<div class="math notranslate nohighlight">
\[\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\]</div>
<div class="math notranslate nohighlight">
\[\delta_t = 3 + 0.9 \times 15.0 - 10.0\]</div>
<div class="math notranslate nohighlight">
\[\delta_t = 3 + 13.5 - 10.0 = 6.5\]</div>
<p class="sd-card-text"><strong>b) Mise à jour TD(0) :</strong></p>
<div class="math notranslate nohighlight">
\[V(S_t) \leftarrow V(S_t) + \alpha \delta_t\]</div>
<div class="math notranslate nohighlight">
\[V(s_1) \leftarrow 10.0 + 0.1 \times 6.5\]</div>
<div class="math notranslate nohighlight">
\[V(s_1) \leftarrow 10.0 + 0.65 = 10.65\]</div>
<p class="sd-card-text"><strong>c) TD error avec <span class="math notranslate nohighlight">\(V(s_2) = 5.0\)</span> :</strong></p>
<div class="math notranslate nohighlight">
\[\delta_t = 3 + 0.9 \times 5.0 - 10.0\]</div>
<div class="math notranslate nohighlight">
\[\delta_t = 3 + 4.5 - 10.0 = -2.5\]</div>
<p class="sd-card-text"><strong>d) Interprétation :</strong></p>
<p class="sd-card-text"><strong>Cas (a)</strong> : <span class="math notranslate nohighlight">\(\delta_t = 6.5 &gt; 0\)</span></p>
<ul class="simple">
<li><p class="sd-card-text">La reward + valeur future (16.5) &gt; estimation actuelle (10.0)</p></li>
<li><p class="sd-card-text">L’état <span class="math notranslate nohighlight">\(s_1\)</span> était <strong>sous-estimé</strong></p></li>
<li><p class="sd-card-text">La mise à jour <strong>augmente</strong> <span class="math notranslate nohighlight">\(V(s_1)\)</span></p></li>
</ul>
<p class="sd-card-text"><strong>Cas (c)</strong> : <span class="math notranslate nohighlight">\(\delta_t = -2.5 &lt; 0\)</span></p>
<ul class="simple">
<li><p class="sd-card-text">La reward + valeur future (7.5) &lt; estimation actuelle (10.0)</p></li>
<li><p class="sd-card-text">L’état <span class="math notranslate nohighlight">\(s_1\)</span> était <strong>surestimé</strong></p></li>
<li><p class="sd-card-text">La mise à jour <strong>diminuerait</strong> <span class="math notranslate nohighlight">\(V(s_1)\)</span></p></li>
</ul>
</div>
</details></section>
<hr class="docutils" />
<section id="exercice-2-sarsa-step-by-step">
<h3>Exercice 2 — SARSA step by step<a class="headerlink" href="#exercice-2-sarsa-step-by-step" title="Link to this heading">#</a></h3>
<div class="tip admonition">
<p class="admonition-title">Exercice</p>
<p>Un agent utilise SARSA avec ε-greedy (<span class="math notranslate nohighlight">\(\varepsilon = 0.2\)</span>).</p>
<p><strong>Transition observée :</strong></p>
<div class="math notranslate nohighlight">
\[S_t = s_A, A_t = a_1 \xrightarrow{R_{t+1} = 5} S_{t+1} = s_B, A_{t+1} = a_2\]</div>
<p><strong>Q-values actuelles :</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Q(s_A, a_1) = 8.0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Q(s_A, a_2) = 6.0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Q(s_B, a_1) = 12.0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Q(s_B, a_2) = 10.0\)</span></p></li>
</ul>
<p><strong>Paramètres :</strong> <span class="math notranslate nohighlight">\(\alpha = 0.2\)</span>, <span class="math notranslate nohighlight">\(\gamma = 0.9\)</span></p>
<p><strong>Questions :</strong></p>
<p>a) Calculez le TD target pour SARSA</p>
<p>b) Calculez le TD error</p>
<p>c) Quelle est la nouvelle valeur <span class="math notranslate nohighlight">\(Q(s_A, a_1)\)</span> ?</p>
<p>d) L’agent a-t-il choisi l’action greedy en <span class="math notranslate nohighlight">\(s_B\)</span> ? Est-ce important pour SARSA ?</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution — Exercice 2</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>a) TD target pour SARSA :</strong></p>
<div class="math notranslate nohighlight">
\[\text{Target} = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1})\]</div>
<div class="math notranslate nohighlight">
\[= 5 + 0.9 \times Q(s_B, a_2)\]</div>
<div class="math notranslate nohighlight">
\[= 5 + 0.9 \times 10.0 = 5 + 9.0 = 14.0\]</div>
<p class="sd-card-text"><strong>b) TD error :</strong></p>
<div class="math notranslate nohighlight">
\[\delta_t = \text{Target} - Q(S_t, A_t)\]</div>
<div class="math notranslate nohighlight">
\[= 14.0 - 8.0 = 6.0\]</div>
<p class="sd-card-text"><strong>c) Mise à jour Q-value :</strong></p>
<div class="math notranslate nohighlight">
\[Q(s_A, a_1) \leftarrow Q(s_A, a_1) + \alpha \delta_t\]</div>
<div class="math notranslate nohighlight">
\[Q(s_A, a_1) \leftarrow 8.0 + 0.2 \times 6.0\]</div>
<div class="math notranslate nohighlight">
\[Q(s_A, a_1) \leftarrow 8.0 + 1.2 = 9.2\]</div>
<p class="sd-card-text"><strong>d) Action greedy en <span class="math notranslate nohighlight">\(s_B\)</span> ?</strong></p>
<p class="sd-card-text">L’action greedy serait <span class="math notranslate nohighlight">\(\arg\max_a Q(s_B, a) = a_1\)</span> (car 12.0 &gt; 10.0).</p>
<p class="sd-card-text">L’agent a choisi <span class="math notranslate nohighlight">\(a_2\)</span>, donc c’était une <strong>action d’exploration</strong> (probabilité <span class="math notranslate nohighlight">\(\varepsilon\)</span>).</p>
<p class="sd-card-text"><strong>Pour SARSA, c’est crucial !</strong> SARSA utilise <span class="math notranslate nohighlight">\(Q(S', A')\)</span> où <span class="math notranslate nohighlight">\(A'\)</span> est l’action <strong>réellement choisie</strong>, donc il apprend la valeur de la policy ε-greedy (qui inclut l’exploration).</p>
</div>
</details></section>
<hr class="docutils" />
<section id="exercice-3-q-learning-vs-sarsa">
<h3>Exercice 3 — Q-Learning vs SARSA<a class="headerlink" href="#exercice-3-q-learning-vs-sarsa" title="Link to this heading">#</a></h3>
<div class="tip admonition">
<p class="admonition-title">Exercice</p>
<p>Même situation que l’exercice 2, mais maintenant comparons Q-Learning.</p>
<p><strong>Rappel de la transition :</strong></p>
<div class="math notranslate nohighlight">
\[S_t = s_A, A_t = a_1 \xrightarrow{R_{t+1} = 5} S_{t+1} = s_B, A_{t+1} = a_2\]</div>
<p><strong>Q-values :</strong> (identiques)</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Q(s_A, a_1) = 8.0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Q(s_B, a_1) = 12.0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Q(s_B, a_2) = 10.0\)</span></p></li>
</ul>
<p><strong>Paramètres :</strong> <span class="math notranslate nohighlight">\(\alpha = 0.2\)</span>, <span class="math notranslate nohighlight">\(\gamma = 0.9\)</span></p>
<p><strong>Questions :</strong></p>
<p>a) Calculez le TD target pour Q-Learning</p>
<p>b) Calculez le TD error</p>
<p>c) Quelle est la nouvelle valeur <span class="math notranslate nohighlight">\(Q(s_A, a_1)\)</span> avec Q-Learning ?</p>
<p>d) Comparez avec le résultat SARSA (exercice 2). Pourquoi la différence ?</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution — Exercice 3</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>a) TD target pour Q-Learning :</strong></p>
<div class="math notranslate nohighlight">
\[\text{Target} = R_{t+1} + \gamma \max_a Q(S_{t+1}, a)\]</div>
<div class="math notranslate nohighlight">
\[= 5 + 0.9 \times \max\{Q(s_B, a_1), Q(s_B, a_2)\}\]</div>
<div class="math notranslate nohighlight">
\[= 5 + 0.9 \times \max\{12.0, 10.0\}\]</div>
<div class="math notranslate nohighlight">
\[= 5 + 0.9 \times 12.0 = 5 + 10.8 = 15.8\]</div>
<p class="sd-card-text"><strong>b) TD error :</strong></p>
<div class="math notranslate nohighlight">
\[\delta_t = 15.8 - 8.0 = 7.8\]</div>
<p class="sd-card-text"><strong>c) Mise à jour Q-value :</strong></p>
<div class="math notranslate nohighlight">
\[Q(s_A, a_1) \leftarrow 8.0 + 0.2 \times 7.8\]</div>
<div class="math notranslate nohighlight">
\[Q(s_A, a_1) \leftarrow 8.0 + 1.56 = 9.56\]</div>
<p class="sd-card-text"><strong>d) Comparaison SARSA vs Q-Learning :</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Méthode</p></th>
<th class="head"><p class="sd-card-text">Target</p></th>
<th class="head"><p class="sd-card-text">Nouvelle <span class="math notranslate nohighlight">\(Q(s_A, a_1)\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text">SARSA</p></td>
<td><p class="sd-card-text">14.0</p></td>
<td><p class="sd-card-text">9.2</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text">Q-Learning</p></td>
<td><p class="sd-card-text">15.8</p></td>
<td><p class="sd-card-text">9.56</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text"><strong>Différence :</strong></p>
<ul class="simple">
<li><p class="sd-card-text"><strong>SARSA</strong> utilise <span class="math notranslate nohighlight">\(Q(s_B, a_2) = 10.0\)</span> (action explorée)</p></li>
<li><p class="sd-card-text"><strong>Q-Learning</strong> utilise <span class="math notranslate nohighlight">\(\max_a Q(s_B, a) = 12.0\)</span> (action optimale)</p></li>
</ul>
<p class="sd-card-text">Q-Learning est plus <strong>optimiste</strong> car il suppose toujours l’action optimale au prochain step, alors que SARSA prend en compte l’exploration réelle.</p>
</div>
</details></section>
<hr class="docutils" />
<section id="exercice-4-sequence-de-mises-a-jour-td">
<h3>Exercice 4 — Séquence de mises à jour TD<a class="headerlink" href="#exercice-4-sequence-de-mises-a-jour-td" title="Link to this heading">#</a></h3>
<div class="tip admonition">
<p class="admonition-title">Exercice</p>
<p>Un agent apprend avec TD(0) sur la séquence suivante :</p>
<p><strong>Épisode :</strong></p>
<div class="math notranslate nohighlight">
\[s_1 \xrightarrow{r=2} s_2 \xrightarrow{r=3} s_3 \xrightarrow{r=5} \text{terminal}\]</div>
<p><strong>Valeurs initiales :</strong> <span class="math notranslate nohighlight">\(V(s_1) = 0\)</span>, <span class="math notranslate nohighlight">\(V(s_2) = 0\)</span>, <span class="math notranslate nohighlight">\(V(s_3) = 0\)</span></p>
<p><strong>Paramètres :</strong> <span class="math notranslate nohighlight">\(\alpha = 0.5\)</span>, <span class="math notranslate nohighlight">\(\gamma = 1.0\)</span></p>
<p><strong>Calculez</strong> les valeurs après une passe <strong>backward</strong> (de la fin vers le début) à travers l’épisode.</p>
<p><strong>Note :</strong> En TD, on met à jour pendant l’épisode, mais pour cet exercice, on simule les mises à jour dans l’ordre inverse pour voir la propagation.</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution — Exercice 4</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>Approche :</strong> On traite les transitions de la fin vers le début.</p>
<p class="sd-card-text"><strong>Transition 3 :</strong> <span class="math notranslate nohighlight">\(s_3 \xrightarrow{r=5} \text{terminal}\)</span></p>
<div class="math notranslate nohighlight">
\[\delta = 5 + 1.0 \times 0 - V(s_3) = 5 - 0 = 5\]</div>
<div class="math notranslate nohighlight">
\[V(s_3) \leftarrow 0 + 0.5 \times 5 = 2.5\]</div>
<p class="sd-card-text"><strong>Transition 2 :</strong> <span class="math notranslate nohighlight">\(s_2 \xrightarrow{r=3} s_3\)</span></p>
<p class="sd-card-text">Utiliser <span class="math notranslate nohighlight">\(V(s_3) = 2.5\)</span> (valeur mise à jour)</p>
<div class="math notranslate nohighlight">
\[\delta = 3 + 1.0 \times 2.5 - V(s_2) = 5.5 - 0 = 5.5\]</div>
<div class="math notranslate nohighlight">
\[V(s_2) \leftarrow 0 + 0.5 \times 5.5 = 2.75\]</div>
<p class="sd-card-text"><strong>Transition 1 :</strong> <span class="math notranslate nohighlight">\(s_1 \xrightarrow{r=2} s_2\)</span></p>
<p class="sd-card-text">Utiliser <span class="math notranslate nohighlight">\(V(s_2) = 2.75\)</span></p>
<div class="math notranslate nohighlight">
\[\delta = 2 + 1.0 \times 2.75 - V(s_1) = 4.75 - 0 = 4.75\]</div>
<div class="math notranslate nohighlight">
\[V(s_1) \leftarrow 0 + 0.5 \times 4.75 = 2.375\]</div>
<p class="sd-card-text"><strong>Valeurs finales après une passe :</strong></p>
<ul class="simple">
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(V(s_1) = 2.375\)</span></p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(V(s_2) = 2.75\)</span></p></li>
<li><p class="sd-card-text"><span class="math notranslate nohighlight">\(V(s_3) = 2.5\)</span></p></li>
</ul>
<p class="sd-card-text"><strong>Observation :</strong> Les valeurs se propagent depuis l’état terminal. <span class="math notranslate nohighlight">\(V(s_1)\)</span> devrait converger vers <span class="math notranslate nohighlight">\(2 + 3 + 5 = 10\)</span> avec plus d’épisodes.</p>
</div>
</details></section>
<hr class="docutils" />
<section id="exercice-5-convergence-avec-learning-rate">
<h3>Exercice 5 — Convergence avec learning rate<a class="headerlink" href="#exercice-5-convergence-avec-learning-rate" title="Link to this heading">#</a></h3>
<div class="tip admonition">
<p class="admonition-title">Exercice</p>
<p>On applique TD(0) sur un état <span class="math notranslate nohighlight">\(s\)</span> avec observations successives :</p>
<p><strong>Séquence de TD targets observés :</strong></p>
<ul class="simple">
<li><p>Step 1 : Target = 12.0</p></li>
<li><p>Step 2 : Target = 8.0</p></li>
<li><p>Step 3 : Target = 10.0</p></li>
<li><p>Step 4 : Target = 11.0</p></li>
</ul>
<p><strong>Valeur initiale :</strong> <span class="math notranslate nohighlight">\(V(s) = 5.0\)</span></p>
<p><strong>Comparez</strong> l’évolution de <span class="math notranslate nohighlight">\(V(s)\)</span> pour deux learning rates :</p>
<p>a) <span class="math notranslate nohighlight">\(\alpha = 0.1\)</span> (petit)</p>
<p>b) <span class="math notranslate nohighlight">\(\alpha = 0.5\)</span> (grand)</p>
<p><strong>Question :</strong> Quel learning rate converge plus vite ? Lequel est plus stable ?</p>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">Solution — Exercice 5</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text"><strong>a) Avec <span class="math notranslate nohighlight">\(\alpha = 0.1\)</span> :</strong></p>
<p class="sd-card-text"><strong>Step 1 :</strong> Target = 12.0
$<span class="math notranslate nohighlight">\(V(s) \leftarrow 5.0 + 0.1 \times (12.0 - 5.0) = 5.0 + 0.7 = 5.7\)</span>$</p>
<p class="sd-card-text"><strong>Step 2 :</strong> Target = 8.0
$<span class="math notranslate nohighlight">\(V(s) \leftarrow 5.7 + 0.1 \times (8.0 - 5.7) = 5.7 + 0.23 = 5.93\)</span>$</p>
<p class="sd-card-text"><strong>Step 3 :</strong> Target = 10.0
$<span class="math notranslate nohighlight">\(V(s) \leftarrow 5.93 + 0.1 \times (10.0 - 5.93) = 5.93 + 0.407 = 6.337\)</span>$</p>
<p class="sd-card-text"><strong>Step 4 :</strong> Target = 11.0
$<span class="math notranslate nohighlight">\(V(s) \leftarrow 6.337 + 0.1 \times (11.0 - 6.337) = 6.337 + 0.466 = 6.803\)</span>$</p>
<p class="sd-card-text"><strong>Séquence :</strong> 5.0 → 5.7 → 5.93 → 6.337 → 6.803</p>
<p class="sd-card-text"><strong>b) Avec <span class="math notranslate nohighlight">\(\alpha = 0.5\)</span> :</strong></p>
<p class="sd-card-text"><strong>Step 1 :</strong> Target = 12.0
$<span class="math notranslate nohighlight">\(V(s) \leftarrow 5.0 + 0.5 \times (12.0 - 5.0) = 5.0 + 3.5 = 8.5\)</span>$</p>
<p class="sd-card-text"><strong>Step 2 :</strong> Target = 8.0
$<span class="math notranslate nohighlight">\(V(s) \leftarrow 8.5 + 0.5 \times (8.0 - 8.5) = 8.5 - 0.25 = 8.25\)</span>$</p>
<p class="sd-card-text"><strong>Step 3 :</strong> Target = 10.0
$<span class="math notranslate nohighlight">\(V(s) \leftarrow 8.25 + 0.5 \times (10.0 - 8.25) = 8.25 + 0.875 = 9.125\)</span>$</p>
<p class="sd-card-text"><strong>Step 4 :</strong> Target = 11.0
$<span class="math notranslate nohighlight">\(V(s) \leftarrow 9.125 + 0.5 \times (11.0 - 9.125) = 9.125 + 0.9375 = 10.0625\)</span>$</p>
<p class="sd-card-text"><strong>Séquence :</strong> 5.0 → 8.5 → 8.25 → 9.125 → 10.0625</p>
<p class="sd-card-text"><strong>Comparaison :</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p class="sd-card-text">Critère</p></th>
<th class="head"><p class="sd-card-text"><span class="math notranslate nohighlight">\(\alpha = 0.1\)</span></p></th>
<th class="head"><p class="sd-card-text"><span class="math notranslate nohighlight">\(\alpha = 0.5\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p class="sd-card-text"><strong>Valeur finale</strong></p></td>
<td><p class="sd-card-text">6.803</p></td>
<td><p class="sd-card-text">10.0625</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><strong>Vitesse</strong></p></td>
<td><p class="sd-card-text">Lente</p></td>
<td><p class="sd-card-text">Rapide</p></td>
</tr>
<tr class="row-even"><td><p class="sd-card-text"><strong>Oscillations</strong></p></td>
<td><p class="sd-card-text">Faibles</p></td>
<td><p class="sd-card-text">Plus importantes</p></td>
</tr>
<tr class="row-odd"><td><p class="sd-card-text"><strong>Stabilité</strong></p></td>
<td><p class="sd-card-text">Élevée</p></td>
<td><p class="sd-card-text">Moyenne</p></td>
</tr>
</tbody>
</table>
</div>
<p class="sd-card-text"><strong>Conclusion :</strong></p>
<ul class="simple">
<li><p class="sd-card-text"><strong><span class="math notranslate nohighlight">\(\alpha\)</span> grand</strong> (0.5) : converge plus vite vers la moyenne des targets (~10.25), mais plus sensible au bruit</p></li>
<li><p class="sd-card-text"><strong><span class="math notranslate nohighlight">\(\alpha\)</span> petit</strong> (0.1) : plus stable, moins d’oscillations, mais convergence plus lente</p></li>
</ul>
<p class="sd-card-text"><strong>Moyenne des targets :</strong> <span class="math notranslate nohighlight">\((12 + 8 + 10 + 11)/4 = 10.25\)</span> — <span class="math notranslate nohighlight">\(\alpha = 0.5\)</span> est plus proche après 4 steps.</p>
</div>
</details></section>
</section>
<hr class="docutils" />
<section id="avantages-et-limitations">
<h2>8. Avantages et limitations<a class="headerlink" href="#avantages-et-limitations" title="Link to this heading">#</a></h2>
<section id="avantages-des-methodes-td">
<h3>8.1 Avantages des méthodes TD<a class="headerlink" href="#avantages-des-methodes-td" title="Link to this heading">#</a></h3>
<div class="note admonition">
<p class="admonition-title">Points forts</p>
<p>✅ <strong>Model-free</strong> : Pas besoin de <span class="math notranslate nohighlight">\(P\)</span> et <span class="math notranslate nohighlight">\(R\)</span></p>
<p>✅ <strong>Apprentissage online</strong> : Mise à jour à chaque step (pas d’attendre la fin de l’épisode)</p>
<p>✅ <strong>Fonctionne avec tâches continues</strong> : Pas besoin d’épisodes terminaux</p>
<p>✅ <strong>Faible variance</strong> : Moins que MC (utilise bootstrap)</p>
<p>✅ <strong>Convergence rapide</strong> : Généralement plus rapide que MC</p>
<p>✅ <strong>Flexible</strong> : Peut être on-policy (SARSA) ou off-policy (Q-Learning)</p>
</div>
</section>
<section id="limitations">
<h3>8.2 Limitations<a class="headerlink" href="#limitations" title="Link to this heading">#</a></h3>
<div class="note admonition">
<p class="admonition-title">Contraintes</p>
<p>❌ <strong>Biais d’estimation</strong> : Bootstrap introduit du biais (contrairement à MC)</p>
<p>❌ <strong>Sensible aux hyperparamètres</strong> : Learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> crucial</p>
<p>❌ <strong>Peut diverger</strong> : Avec function approximation (voir deadly triad)</p>
<p>❌ <strong>Exploration nécessaire</strong> : Tous états doivent être visités</p>
<p>❌ <strong>Q-Learning peut surestimer</strong> : Maximization bias (résolu par Double Q-Learning)</p>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./theorie"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="03_monte_carlo.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Monte Carlo Methods</p>
      </div>
    </a>
    <a class="right-next"
       href="05_function_approximation.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">06 — Approximation de Fonctions &amp; Généralisation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#principe-fondamental-du-td-learning">1. Principe fondamental du TD Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#td-error">1.1 TD Error</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#td-0-prediction">2. TD(0) — Prediction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithme">2.1 Algorithme</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#proprietes-de-convergence">2.2 Propriétés de convergence</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#td-control-sarsa-on-policy">3. TD Control : SARSA (On-policy)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#principe">3.1 Principe</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">3.2 Algorithme</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#caracteristiques">3.3 Caractéristiques</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#q-learning-off-policy">4. Q-Learning (Off-policy)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">4.1 Principe</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">4.2 Algorithme</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convergence">4.3 Convergence</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sarsa-vs-q-learning">5. SARSA vs Q-Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparaison-theorique">5.1 Comparaison théorique</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exemple-illustratif-cliff-walking">5.2 Exemple illustratif : Cliff Walking</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#td-et-eligibility-traces">6. TD(λ) et Eligibility Traces</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#n-step-td">6.1 N-step TD</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#td-avec-eligibility-traces">6.2 TD(λ) avec eligibility traces</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercices-theoriques">7. Exercices théoriques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercice-1-calcul-de-td-error">Exercice 1 — Calcul de TD error</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercice-2-sarsa-step-by-step">Exercice 2 — SARSA step by step</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercice-3-q-learning-vs-sarsa">Exercice 3 — Q-Learning vs SARSA</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercice-4-sequence-de-mises-a-jour-td">Exercice 4 — Séquence de mises à jour TD</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercice-5-convergence-avec-learning-rate">Exercice 5 — Convergence avec learning rate</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#avantages-et-limitations">8. Avantages et limitations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#avantages-des-methodes-td">8.1 Avantages des méthodes TD</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations">8.2 Limitations</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Béria C. Kalpélbé
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>